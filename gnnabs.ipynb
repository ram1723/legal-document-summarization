{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of GPU: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Name: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "\n",
    "# Load Legal-BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Function to clean legal text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', '[DATE]', text)  # Replace dates\n",
    "    text = re.sub(r'\\b\\d{4}\\b', '[YEAR]', text)  # Replace years\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetical characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    text = re.sub(r'\\bAIR\\s\\d{4}\\sSC\\s\\d{3,4}\\b', '[CITATION]', text)  # Replace case citations\n",
    "    \n",
    "    # Replace legal terms with simpler terms\n",
    "    legal_dict = {\n",
    "        'hereinabove': 'above',\n",
    "        'hereinafter': 'below',\n",
    "        'plaintiff': 'claimant',\n",
    "        'defendant': 'respondent'\n",
    "    }\n",
    "    for term, replacement in legal_dict.items():\n",
    "        text = text.replace(term, replacement)\n",
    "    \n",
    "    # Remove boilerplate legal phrases\n",
    "    boilerplate_phrases = [\n",
    "        'the learned counsel submitted that',\n",
    "        'in light of the above discussion',\n",
    "        'the facts of the case are as follows'\n",
    "    ]\n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to read file content\n",
    "def read(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        return text\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    judgment_text_path = file_path[0].numpy().decode('utf-8')\n",
    "\n",
    "    # Read and clean the legal judgment text\n",
    "    judgment_text = read(judgment_text_path)\n",
    "    cleaned_judgment_text = clean_text(judgment_text)\n",
    "\n",
    "    # Tokenization of the cleaned judgment text using Legal-BERT tokenizer\n",
    "    original_text_tokened = tokenizer(cleaned_judgment_text, return_tensors='tf', truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "    # Generate embeddings from Legal-BERT\n",
    "    embeddings = model(original_text_tokened)[0]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "    \n",
    "    return original_text_tokened['input_ids'], original_text_tokened['attention_mask'], embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"C:/Users/prasa/Downloads/7152317/dataset/dataset/IN-Abs\"\n",
    "train_judgement_dir = os.path.join(dataset_dir, 'train-data', 'judgement')\n",
    "train_summary_dir = os.path.join(dataset_dir, 'train-data', 'summary')\n",
    "\n",
    "# Prepare file paths for judgments and summaries\n",
    "train_files = [(os.path.join(train_judgement_dir, file), os.path.join(train_summary_dir, file)) for file in os.listdir(train_judgement_dir)]\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "train_dataset = train_dataset.map(lambda x: tf.py_function(load_and_preprocess_data, [x], [tf.int32, tf.int32, tf.int32]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_files))\n",
    "train_dataset = train_dataset.padded_batch(16, padded_shapes=([None], [None], [None]))\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "docs = []\n",
    "for file_name in os.listdir(train_judgement_dir):\n",
    "    file_path = os.path.join(train_judgement_dir, file_name)\n",
    "    text = read(file_path)\n",
    "    docs.append(text)\n",
    "stopwords=stopwords.words('english')\n",
    "vectorizer=CountVectorizer(max_df=0.9, min_df=2, stop_words=stopwords)\n",
    "term_matrix=vectorizer.fit_transform(docs)\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(term_matrix)\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    print(\" \".join(terms[i] for i in topic.argsort()[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def graph_build(docs, term_matrix, tfidf_matrix, lda_model, terms):\n",
    "    G = nx.Graph()\n",
    "    inputs=tokenizer(docs, return_tensors='tf',truncation=True,padding='max_length',max_length=512)\n",
    "    outputs=model(inputs)\n",
    "    sen_embeddings=outputs.last_hidden_state.mean(axis=1)\n",
    "    for idx,doc in enumerate(docs):\n",
    "        G.add_node(doc,embedding=sen_embeddings[idx].numpy(),type='sentence')\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        G.add_node(f\"topic_{topic_idx}\", type='topic', words=[terms[i] for i in topic.argsort()[-10:]])\n",
    "    for tfidf_idx, doc_tfidf in enumerate(tfidf_matrix):\n",
    "        feature_index = doc_tfidf.nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [doc_tfidf[0, x] for x in feature_index])\n",
    "        for word_idx, score in tfidf_scores:\n",
    "            word = terms[word_idx]\n",
    "            # Find topic containing this word and connect\n",
    "            for topic_idx, topic_words in enumerate([terms[i] for i in topic.argsort()[-10:]] for topic in lda_model.components_):\n",
    "                if word in topic_words:\n",
    "                    G.add_edge(f\"sentence_{tfidf_idx}\", f\"topic_{topic_idx}\", weight=score)\n",
    "    return G\n",
    "graph=graph_build(docs, term_matrix, tfidf_matrix, lda_model, terms)\n",
    "print(nx.info(graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_dim), initializer='random_normal', trainable=True)\n",
    "        self.a = self.add_weight(shape=(2 * self.output_dim, 1), initializer='random_normal', trainable=True)\n",
    "    def call(self, node_features, adj_matrix):\n",
    "        node_features_transformed = tf.matmul(node_features, self.W)\n",
    "        N = tf.shape(node_features)[0]  # Number of nodes\n",
    "        attention_scores = []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if adj_matrix[i][j] > 0:  # If nodes are connected\n",
    "                    concatenated_features = tf.concat([node_features_transformed[i], node_features_transformed[j]], axis=-1)\n",
    "                    score = tf.nn.leaky_relu(tf.matmul(concatenated_features, self.a))\n",
    "                    attention_scores.append((i, j, score))\n",
    "        adj_matrix = tf.convert_to_tensor(adj_matrix, dtype=tf.float32)\n",
    "        attention_scores_softmax = tf.nn.softmax(adj_matrix)\n",
    "        updated_node_features = []\n",
    "        for i in range(N):\n",
    "            neighbors = [j for j in range(N) if adj_matrix[i][j] > 0]\n",
    "            weighted_sum = tf.zeros(self.output_dim)\n",
    "            for j in neighbors:\n",
    "                weight = attention_scores_softmax[i][j]\n",
    "                weighted_sum += weight * node_features_transformed[j]\n",
    "            updated_node_features.append(weighted_sum)\n",
    "\n",
    "        return tf.convert_to_tensor(updated_node_features)\n",
    "def graph_to_adj_matrix(graph):\n",
    "    adj_matrix = nx.to_numpy_matrix(graph)\n",
    "    return adj_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix=graph_to_adj_matrix(graph)\n",
    "gat_layer=GAT(output_dim=128,num_heads=1)\n",
    "node_features = np.random.rand(len(graph.nodes), 768)\n",
    "node_features = tf.convert_to_tensor(node_features, dtype=tf.float32)\n",
    "updated_node_features = gat_layer(node_features, adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "def attention_pooling(embeddings):\n",
    "  attention_weights = tf.nn.softmax(tf.keras.layers.Lambda(lambda x: x)(embeddings), axis=1)\n",
    "  attention_weights = tf.cast(attention_weights, dtype=embeddings.dtype)\n",
    "  pooled_embeddings = tf.reduce_sum(embeddings * attention_weights, axis=1)\n",
    "  return pooled_embeddings\n",
    "\n",
    "# Custom Loss Function (same as before)\n",
    "def custom_loss(labels, y_pred, original_embeddings, summary_embeddings):\n",
    "    cross_entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, y_pred)\n",
    "    original_embeddings_pooled = attention_pooling(original_embeddings)\n",
    "    summary_embeddings_pooled = attention_pooling(summary_embeddings)\n",
    "    original_embeddings_norm = tf.nn.l2_normalize(original_embeddings_pooled, axis=-1)\n",
    "    summary_embeddings_norm = tf.nn.l2_normalize(summary_embeddings_pooled, axis=-1)\n",
    "    cosine_sim = tf.reduce_sum(tf.multiply(original_embeddings_norm, summary_embeddings_norm), axis=-1)\n",
    "    cosine_sim_loss = tf.reduce_mean(1 - cosine_sim)\n",
    "    combined_loss = cross_entropy_loss + cosine_sim_loss\n",
    "    return combined_loss\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "bart_model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bart_model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(bart_model_name)\n",
    "\n",
    "# Training Step using BART Decoder with GAT Embeddings\n",
    "@tf.function\n",
    "def train_step_gat(input_embeddings, decoder_input_ids, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: GAT embeddings (bypassing BART encoder)\n",
    "        outputs = model.model.decoder(input_ids=decoder_input_ids, encoder_hidden_states=input_embeddings, training=True).logits\n",
    "        \n",
    "        # Custom Loss: Cross-Entropy + Cosine Similarity Loss\n",
    "        loss = custom_loss(labels, outputs, input_embeddings, outputs)\n",
    "\n",
    "        # Mixed precision loss scaling\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "\n",
    "    # Compute gradients\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "\n",
    "# Initialize optimizer\n",
    "base_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Example training loop with correct dataset usage\n",
    "epochs = 10\n",
    "\n",
    "# Assuming `updated_node_features` is the input embeddings from the GAT layer\n",
    "for epoch in range(epochs):\n",
    "    for (batch, (input_ids, attention_mask, decoder_input_ids)) in enumerate(train_dataset):\n",
    "        labels = decoder_input_ids[:, 1:]  # Shifted for the decoder\n",
    "        decoder_input_ids = decoder_input_ids[:, :-1]\n",
    "\n",
    "        # Use the GAT embeddings (assume precomputed) as input\n",
    "        input_embeddings = updated_node_features  # These embeddings are from the GAT layer for the current batch\n",
    "\n",
    "        # Train with GAT embeddings and decoder inputs (summary)\n",
    "        train_step_gat(input_embeddings, decoder_input_ids, labels, optimizer)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss.result()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
