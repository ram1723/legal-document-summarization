{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lexnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace dates and years with placeholders\n",
    "    text = re.sub(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', '[DATE]', text)\n",
    "    text = re.sub(r'\\b\\d{4}\\b', '[YEAR]', text)\n",
    "    \n",
    "    # Remove special characters and unwanted symbols\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Replace specific legal citations\n",
    "    text = re.sub(r'\\bAIR\\s\\d{4}\\sSC\\s\\d{3,4}\\b', '[CASE CITATION]', text)\n",
    "    \n",
    "    # Standardize legal terminology\n",
    "    legal_dict = {\n",
    "        'hereinabove': 'above',\n",
    "        'hereinafter': 'below',\n",
    "        'plaintiff': 'claimant',\n",
    "        'defendant': 'respondent',\n",
    "        'learned counsel': 'lawyer',\n",
    "        'aforesaid': 'previously mentioned',\n",
    "        'writ petition': 'legal petition'}\n",
    "    for term, replacement in legal_dict.items():\n",
    "        text = text.replace(term, replacement)\n",
    "    \n",
    "    boilerplate_phrases = [\n",
    "        'the learned counsel submitted that',\n",
    "        'in light of the above discussion',\n",
    "        'the facts of the case are as follows',\n",
    "    ]\n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    return sentences\n",
    "def read(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        return text\n",
    "def load_and_preprocess_data(file_path):\n",
    "    judgment_text_path = file_path[0].numpy().decode('utf-8')\n",
    "    judgment_text = read(judgment_text_path)\n",
    "    cleaned_judgment_text_tokenized = clean_text(judgment_text)\n",
    "    return load_and_preprocess_data\n",
    "dataset_dir = \"C:/Users/prasa/Downloads/7152317/dataset/dataset/IN-Abs\"\n",
    "train_judgement_dir = os.path.join(dataset_dir, 'train-data', 'judgement')\n",
    "preprocessed_data=load_and_preprocess_data(train_judgement_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexnlp\n",
    "def text_extraction_entities_relations_lexnlp(text):\n",
    "    entities = list(lexnlp.extract.en.entities.nltk_re.get_persons(text))\n",
    "    statutes = list(lexnlp.extract.en.acts.get_acts(text))\n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"statutes\": statutes\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LDA(Latent Dirichlet Allocation_Topic modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "docs = []\n",
    "for file_name in os.listdir(train_judgement_dir):\n",
    "    file_path = os.path.join(train_judgement_dir, file_name)\n",
    "    text = read(file_path)\n",
    "    docs.append(text)\n",
    "stopwords=stopwords.words('english')\n",
    "vectorizer=CountVectorizer(max_df=0.9, min_df=2, stop_words=stopwords)\n",
    "term_matrix=vectorizer.fit_transform(docs)\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(term_matrix)\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    print(\" \".join(terms[i] for i in topic.argsort()[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_keywords = ['contract','agreement','performance','plaintiff']\n",
    "issue_keywords = ['issue','dispute','claim','damages']\n",
    "def filter_topics(terms, topics, fact_keywords, issue_keywords):\n",
    "    facts=[]\n",
    "    issues=[]\n",
    "    for idx, topic in enumerate(topics):\n",
    "        topic_words = \" \".join(terms[i] for i in topic.argsort()[-10:])\n",
    "        if any(keyword in topic_words for keyword in fact_keywords):\n",
    "            facts.append((idx, topic_words))\n",
    "        elif any(keyword in topic_words for keyword in issue_keywords):\n",
    "            issues.append((idx, topic_words))\n",
    "    return facts, issues\n",
    "topics = lda_model.components_\n",
    "facts, issues = filter_topics(terms, topics, fact_keywords, issue_keywords)\n",
    "# print(\"Facts:\")\n",
    "# for fact in facts:\n",
    "#     print(fact)\n",
    "\n",
    "# print(\"\\nIssues:\")\n",
    "# for issue in issues:\n",
    "#     print(issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='tf', truncation=True, max_length=512)\n",
    "    outputs = model(inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def graph(facts,issues,entity_data):\n",
    "    G = nx.DiGraph()\n",
    "    for idx, fact in facts:\n",
    "        fact_embedding = get_embedding(fact[1])\n",
    "        G.add_node(f'fact_{idx}', embedding=fact_embedding, type='fact')\n",
    "    for idx, issue in issues:\n",
    "        issue_embedding = get_embedding(issue[1])\n",
    "        G.add_node(f'issue_{idx}', embedding=issue_embedding, type='issue')\n",
    "    for entity in entity_data['entities']:\n",
    "        entity_embedding = get_embedding(entity)\n",
    "        G.add_node(entity, embedding=entity_embedding, type='entity')\n",
    "    for statute in entity_data['statutes']:\n",
    "        statute_embedding = get_embedding(statute)\n",
    "        G.add_node(statute, embedding=statute_embedding, type='statute')\n",
    "    for fact_node in G.nodes(data=True):\n",
    "        if fact_node[1]['type'] == 'fact':\n",
    "            for issue_node in G.nodes(data=True):\n",
    "                if issue_node[1]['type'] == 'issue':\n",
    "                    G.add_edge(fact_node[0], issue_node[0], relationship='related')\n",
    "    for entity_node in G.nodes(data=True):\n",
    "        if entity_node[1]['type'] == 'entity':\n",
    "            for fact_node in G.nodes(data=True):\n",
    "                if fact_node[1]['type'] == 'fact':\n",
    "                    G.add_edge(entity_node[0], fact_node[0], relationship='related to fact')\n",
    "            for issue_node in G.nodes(data=True):\n",
    "                if issue_node[1]['type'] == 'issue':\n",
    "                    G.add_edge(entity_node[0], issue_node[0], relationship='related to issue')\n",
    "    for statute_node in G.nodes(data=True):\n",
    "        if statute_node[1]['type'] == 'statute':\n",
    "            for fact_node in G.nodes(data=True):\n",
    "                if fact_node[1]['type'] == 'fact':\n",
    "                    G.add_edge(statute_node[0], fact_node[0], relationship='cited in fact')\n",
    "            for issue_node in G.nodes(data=True):\n",
    "                if issue_node[1]['type'] == 'issue':\n",
    "                    G.add_edge(statute_node[0], issue_node[0], relationship='cited in issue')\n",
    "\n",
    "    return G\n",
    "entity_data = text_extraction_entities_relations_lexnlp(docs[0])\n",
    "graph = graph(facts, issues, entity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train EUGAT model fro better retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "node_embeddings = []\n",
    "for node in graph.nodes(data=True):\n",
    "    node_embeddings.append(node[1]['embedding'])\n",
    "node_inputs = tf.convert_to_tensor(node_embeddings, dtype=tf.float32)\n",
    "edge_features = []\n",
    "relationship_types = {\n",
    "    'related': [1, 0, 0],            # Fact to Issue relationship\n",
    "    'related to fact': [0, 1, 0],    # Entity to Fact relationship\n",
    "    'related to issue': [0, 0, 1]    # Entity to Issue relationship\n",
    "}\n",
    "for edge in graph.edges(data=True):\n",
    "    relationship = edge[2]['relationship']\n",
    "    edge_features.append(relationship_types.get(relationship, [0, 0, 0])) \n",
    "edge_inputs = tf.convert_to_tensor(edge_features, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "class EUGATLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=4, **kwargs):\n",
    "        super(EUGATLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "    def build(self, input_shape):\n",
    "        self.node_weights = self.add_weight(\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "        \n",
    "        self.edge_weights = self.add_weight(\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "        \n",
    "        self.attention_heads = self.add_weight(\n",
    "            shape=(self.num_heads, self.output_dim, self.output_dim),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "    def call(self, node_inputs, edge_inputs):\n",
    "        # Node features projection\n",
    "        node_features = tf.matmul(node_inputs, self.node_weights)\n",
    "        edge_features = tf.matmul(edge_inputs, self.edge_weights)\n",
    "        for head in range(self.num_heads):\n",
    "            attn_weights = tf.matmul(node_features, self.attention_heads[head])\n",
    "            attn_scores = tf.nn.softmax(attn_weights)\n",
    "            node_updates = tf.matmul(attn_scores, edge_features)\n",
    "        \n",
    "        return node_updates\n",
    "class EUGATModel(tf.keras.Model):\n",
    "    def __init__(self, output_dim, num_heads=4):\n",
    "        super(EUGATModel, self).__init__()\n",
    "        self.gat_layer = EUGATLayer(output_dim, num_heads)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        node_inputs, edge_inputs = inputs\n",
    "        return self.gat_layer(node_inputs, edge_inputs)\n",
    "output_dim = 128\n",
    "model = EUGATModel(output_dim=output_dim)\n",
    "model.compile(optimizer='adam')\n",
    "model.fit([node_inputs, edge_inputs], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = get_embedding(new_case_text)\n",
    "import tensorflow.keras.backend as K\n",
    "def cosine_similarity(a, b):\n",
    "    return K.sum(a * b, axis=-1) / (K.sqrt(K.sum(a * a, axis=-1)) * K.sqrt(K.sum(b * b, axis=-1)))\n",
    "similarities = cosine_similarity(query_embedding, node_inputs)\n",
    "most_similar_cases = np.argsort(similarities.numpy())[::-1][:5]\n",
    "for case_index in most_similar_cases:\n",
    "    print(f\"Case {case_index}:\")\n",
    "    print(docs[case_index])  # Retrieve the original text from the dataset\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
